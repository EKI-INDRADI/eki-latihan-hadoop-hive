su - <USER_HIDDEN>

cd /home/<USER_HIDDEN>
wget https://downloads.apache.org/hive/hive-2.3.7/apache-hive-2.3.7-bin.tar.gz

tar xzfv apache-hive-2.3.7-bin.tar.gz
nano .bashrc

export HIVE_HOME="/home/<USER_HIDDEN>/apache-hive-2.3.7-bin"
export PATH=$PATH:$HIVE_HOME/bin
# Java
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
# Hadoop Related Options
export HADOOP_HOME=/home/<USER_HIDDEN>/hadoop-2.7.7


source ~/.bashrc

copy paste -----------
export HIVE_HOME="/home/<USER_HIDDEN>/apache-hive-2.3.7-bin" \
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 \
export HADOOP_HOME=/home/<USER_HIDDEN>/hadoop-2.7.7 \
export PATH=$PATH:$HIVE_HOME/bin
copy paste -----------


nano $HIVE_HOME/bin/hive-config.sh
# HIVE
export HIVE_HOME="/home/<USER_HIDDEN>/apache-hive-2.3.7-bin" 
# Java
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
# Hadoop Related Options
export HADOOP_HOME=/home/<USER_HIDDEN>/hadoop-2.7.7


--- wajib jalanin hadop  untuk hdfs
https://stackoverflow.com/questions/50968183/java-net-connectexception-your-endpoint-configuration-is-wrong
cd $HADOOP_HOME/sbin

./start-dfs.sh
./start-yarn.sh
--------
https://programmer.group/hadoop-2.9.1-install-hive-2.3.3-on-ubuntu-16.04.html


hadoop fs -mkdir /user
hadoop fs -mkdir /user/hive237
hadoop fs -mkdir /user/hive237/warehouse
exit
sudo -i
mkdir /user/hive237/temp_hive237
chmod 777 -Rv /user/hive237/temp_hive237

hadoop fs -mkdir  /user/hive237/temp_hive237
hadoop fs -chmod  777 /user/hive237/warehouse
hadoop fs -chmod  777 /user/hive237/temp_hive237 <<<<<<<<<<<<<<< waspada hilang



----------- NOT USED
hdfs dfs -mkdir /tmp
hdfs dfs -chmod g+w /tmp
hdfs dfs -ls /

hdfs dfs -mkdir -p /user/hive/warehouse
hdfs dfs -chmod g+w /user/hive/warehouse
hdfs dfs -ls /user/hive
----------- NOT USED



cd $HIVE_HOME/conf

cp hive-default.xml.template hive-site.xml

 nano hive-site.xml

--- pastikan sama
<property>
    <name>hive.metastore.warehouse.dir</name>
    <value>/user/hive237/warehouse</value>
    <description>location of default database for the warehouse</description>
</property>

<property>
    <name>hive.exec.scratchdir</name>
    <value>/user/hive237/temp_hive237</value>   --- sebelumnya ada di /tmp atau /tmp/hive237
    <description>HDFS root scratch dir for Hive jobs which gets created with write all (733) permission. For each connecting user, an HDFS scratch dir: ${hive.exec.scratchdir}/&lt;username&gt; is created, with ${hive.scratch.dir.permission}.</description>
</property>

----------------

ubah semua /user/hive
jadi /user/hive237


hdfs namenode -format
hdfs dfsadmin -safemode leave --- not use
hadoop namenode -format 

rm -rf $HIVE_HOME/conf/metastore_db
$HIVE_HOME/bin/schematool -initSchema -dbType derby

--------- fix err
Logging initialized using configuration in jar:file:/usr/local/apache-hive-2.3.3-bin/lib/hive-common-2.3.3.jar!/hive-log4j2.properties Async: true
Exception in thread "main" java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: ${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D
        at org.apache.hadoop.fs.Path.initialize(Path.java:254)
        at org.apache.hadoop.fs.Path.<init>(Path.java:212)
        at org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:659)
        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:582)
        at org.apache.hadoop.hive.ql.session.SessionState.beginStart(SessionState.java:549)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:750)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:686)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:239)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:153)
Caused by: java.net.URISyntaxException: Relative path in absolute URI: ${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D
        at java.net.URI.checkPath(URI.java:1823)
        at java.net.URI.<init>(URI.java:745)
        at org.apache.hadoop.fs.Path.initialize(Path.java:251)
        ... 12 more




mkdir /home/<USER_HIDDEN>/apache-hive-2.3.7-bin/tmpdir

nano $HIVE_HOME/conf/hive-site.xml

<property>
     <name>system:java.io.tmpdir</name>
     <value>/home/<USER_HIDDEN>/apache-hive-2.3.7-bin/tmpdir</value>
  </property>
  <property>
     <name>system:user.name</name>
     <value>hive</value>
</property>



--------- fix err




jika error Hive error : 'java.lang.RuntimeException(Error caching map.xml: java.io.InterruptedIOException:)

masalah di /user/hive237/temp_hive237  << ikut ke hapus saat clear hdf

hrs mkdir ulang




























//=================== SKIP THIS ERR TETEP RUNNING
--error 1

https://stackoverflow.com/questions/27050820/running-hive-0-12-with-error-of-slf4j

 Class path contains multiple SLF4J bindings.
--error 1

-- solved err 1
find . -name "*hive-jdbc*"
find . -name "*log4j*"

cd $HIVE_HOME/

mkdir $HIVE_HOME/BACKUP_DEL

mv ./lib/log4j-slf4j-impl-2.10.0.jar ./BACKUP_DEL/

rm ./lib/log4j-slf4j-impl-2.10.0.jar

xxxxxxxxxxx NOT USE SKIP THIS PROSESS
mv $HIVE_HOME/metastore_db  ./BACKUP_DEL/
rm -Rf $HIVE_HOME/metastore_db  ./BACKUP_DEL/

mv ./lib/hive-jdbc-3.1.2.jar ./BACKUP_DEL/
rm ./lib/hive-jdbc-3.1.2.jar
xxxxxxxxxxx NOT USE SKIP THIS PROSESS

-- solved err 1

//=================== SKIP THIS ERR TETEP RUNNING

$HIVE_HOME/bin/schematool -initSchema -dbType derby

--error 2
Exception in thread "main" java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V
--error 2

-- solved err 2
mkdir $HIVE_HOME/BACKUP_DEL

cd /home/<USER_HIDDEN>
find . -name "*guava*"



--resultna

./hadoop-2.7.7/share/hadoop/common/lib/guava-27.0-jre.jar
./hadoop-2.7.7/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar
./hadoop-2.7.7/share/hadoop/hdfs/lib/guava-27.0-jre.jar
./hadoop-2.7.7/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar
./apache-hive-2.3.7-bin/lib/jersey-guava-2.25.1.jar
./apache-hive-2.3.7-bin/lib/guava-19.0.jar

--

mv ./apache-hive-2.3.7-bin/lib/guava-19.0.jar ./apache-hive-2.3.7-bin/BACKUP_DEL/

cp ./hadoop-2.7.7/share/hadoop/hdfs/lib/guava-27.0-jre.jar ./apache-hive-2.3.7-bin/lib/
atau
cp $HADOOP_HOME/share/hadoop/hdfs/lib/guava-27.0-jre.jar $HIVE_HOME/lib/



$HIVE_HOME/bin/schematool -initSchema -dbType derby

-- solved err 2




--error 3
Exception in thread "main" java.lang.RuntimeException: com.ctc.wstx.exc.WstxParsingException: Illegal character entity: expansion character (code 0x8
 at [row,col,system-id]: [3215,96,"file:/home/<USER_HIDDEN>/apache-hive-2.3.7-bin/conf/hive-site.xml"]

--error 3


-- solved err 3
https://stackoverflow.com/questions/52783323/hive-throws-wstxparsingexception-illegal-character-entity-expansion-character

nano /home/<USER_HIDDEN>/apache-hive-2.3.7-bin/conf/hive-site.xml
shift+alt+3

di baris 3215,96

ada  ini ===> r&#8;    hapus itu alay


     <description>
3215       Ensures commands with OVERWRITE (such as INSERT OVERWRITE) acquire Exclusive locks for&#8;transactional tables.  This ensures that inserts (w/o o>
3216       are not hidden by the INSERT OVERWRITE.
3217     </description>


$HIVE_HOME/bin/schematool -initSchema -dbType derby

---- selesai
Initialization script completed
schemaTool completed
----- selesai



cd $HIVE_HOME/bin

hive
-- solved err 3






-- error 4

<USER_HIDDEN>@srv-u2004-hive:~/apache-hive-2.3.7-bin/bin$ hive
Hive Session ID = a420ad88-2f14-41fe-8f60-0da1afac5492

Logging initialized using configuration in jar:file:/home/<USER_HIDDEN>/apache-hive-2.3.7-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Exception in thread "main" java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: ${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D
        at org.apache.hadoop.fs.Path.initialize(Path.java:263)
        at org.apache.hadoop.fs.Path.<init>(Path.java:221)
        at org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:710)
        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:627)
        at org.apache.hadoop.hive.ql.session.SessionState.beginStart(SessionState.java:591)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:747)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:323)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:236)
Caused by: java.net.URISyntaxException: Relative path in absolute URI: ${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D
        at java.net.URI.checkPath(URI.java:1823)
        at java.net.URI.<init>(URI.java:745)
        at org.apache.hadoop.fs.Path.initialize(Path.java:260)
        ... 12 more

-- error 4

-- solved err 4
https://stackoverflow.com/questions/27099898/java-net-urisyntaxexception-when-starting-hive

nano /home/<USER_HIDDEN>/apache-hive-2.3.7-bin/conf/hive-site.xml
tamabh di paling atas

  <property>
    <name>system:java.io.tmpdir</name>
    <value>/tmp/hive/java</value>
  </property>
  <property>
    <name>system:user.name</name>
    <value>${user.name}</value>
  </property>



cd $HIVE_HOME/bin

hive


solved . . . . .
-- solved err 4





-- err 5
java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
-- err 5
-- solved err 5
https://stackoverflow.com/questions/35449274/java-lang-runtimeexception-unable-to-instantiate-org-apache-hadoop-hive-ql-meta


nano ~/.bashrc
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export PATH="$PATH:$JAVA_HOME/bin"

cd $HIVE_HOME/bin

hive
-------------------------
SHOW DATABASES;
CREATE SCHEMA eki_latihan_db;
use eki_latihan_db;

-------------------------
CREATE TABLE IF NOT EXISTS employee ( eid int, name String,
salary String, destination String)
COMMENT 'Employee details'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;
-------------------------


select count(*) from employee;


INSERT INTO TABLE employee VALUES (1, 'eki', '10000','asdasdsa'); 
INSERT INTO TABLE employee VALUES (1, 'eki', '10000','asdasdsa'), (1, 'eki2', '100002','asdasdsa'); 


https://stackoverflow.com/questions/46268896/java-lang-runtimeexception-org-apache-hadoop-hive-ql-metadata-hiveexception-or







-- err 6
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

java.lang.RuntimeException: Error caching map.xml


-- err 6





----------- ERROR INSERT --

In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
java.lang.RuntimeException: Error caching map.xml



set hive.exec.dynamic.partitions = true
set hive.exec.dynamic.partitions.mode = nonstrict


https://knowledge.informatica.com/s/article/489870

----------- ERROR INSERT---


-- solved err 6
https://community.cloudera.com/t5/Support-Questions/hive-set-map-reduce-tasks-not-working/td-p/216814
set map.reduce.tasks=32

https://stackoverflow.com/questions/16203314/how-does-hive-choose-the-number-of-reducers-for-a-job

https://www.bbvadata.com/self-service-performance-tuning-for-hive/

https://stackoverflow.com/questions/8762064/hive-unable-to-manually-set-number-of-reducers


  set hive.exec.reducers.bytes.per.reducer=<number>
  set hive.exec.reducers.max=<number>
  set mapreduce.job.reduces=<number>



https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=42565937

------ recomend
set hive.exec.reducers.bytes.per.reducer=256000000;
set hive.exec.reducers.max=1009 ;
set mapreduce.job.reduces= 1 ;
------ recomend
set mapreduce.job.reduces= -1 https://stackoverflow.com/questions/41628765/can-anyone-please-explain-what-does-mapreduce-job-reduces-1-means

set hive.exec.reducers.bytes.per.reducer=1000000000
set hive.exec.reducers.max=999
set mapreduce.job.reduces= 1 http://hadoop.apache.org/docs/r2.7.1/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml



-------------------jika masih error
cd $HADOOP_HOME/sbin

./start-dfs.sh
./start-yarn.sh

./stop-all.sh
--------------------not use
/home/<USER_HIDDEN>/hive-2.3.7-bin/   <<<<<< MASALAH harusya apache-hive-2.3.7-bin/
--------------------not use
b
rm -rf $HIVE_HOME/metastore_db

cd $HIVE_HOME
schematool -initSchema -dbType derby







-------------------jika masih error



-- solved err 5



-------------------



https://www.ibm.com/support/pages/resolving-exceeded-max-jobconf-size-error-hive


find . -name "*mapred-site.xml*"

find . -name "*mapred-site*"




nano /home/<USER_HIDDEN>/hadoop-2.7.7/etc/hadoop/mapred-site.xml

<property>
<name>mapred.user.jobconf.limit</name>
<value>10485760</value>
</property>


cd $HADOOP_HOME/sbin

./stop-dfs.sh
./stop-yarn.sh




----------------------


FIX ERROR
SemanticException org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient



https://askubuntu.com/questions/1073035/failed-hiveexception-java-lang-runtimeexception-unable-to-instantiate-org-apac

sudo apt install derby-tools libderby-java libderbyclient-java








nano /home/<USER_HIDDEN>/apache-hive-2.3.7-bin/conf/hive-site.xml
<property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:derby:/home/<USER_HIDDEN>/apache-hive-2.3.7-bin/metastore_db;databaseName=metastore_db;create=true</value>
</property>


defaultnya :
<property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:derby:;databaseName=metastore_db;create=true</value>
    <description>
      JDBC connect string for a JDBC metastore.
      To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL.
      For example, jdbc:postgresql://myhost/db?ssl=true for postgres database.
    </description>
  </property>


rm -rf $HIVE_HOME/metastore_db

$HIVE_HOME/bin/schematool -initSchema -dbType derby




cd $HIVE_HOME/bin

hive
-------------------------
SHOW DATABASES;
CREATE SCHEMA eki_latihan_db;
use eki_latihan_db;

-------------------------
CREATE TABLE IF NOT EXISTS employee ( eid int, name String,
salary String, destination String)
COMMENT 'Employee details'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;
-------------------------


select count(*) from employee;


INSERT INTO TABLE employee VALUES (1, 'eki', '10000','asdasdsa'); 
INSERT INTO TABLE employee VALUES (1, 'eki', '10000','asdasdsa'), (1, 'eki2', '100002','asdasdsa'); 

------------------ Enable acid support -----------------------
https://www.hdfstutorial.com/blog/update-delete-hive-tables/
http://unmeshasreeveni.blogspot.com/2014/11/updatedeleteinsert-in-hive-0140.html
https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties
https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=82903061#ConfigurationProperties-hive.txn.manager



nano /home/<USER_HIDDEN>/apache-hive-2.3.7-bin/conf/hive-site.xml

 hive.support.concurrency                                     true   , defaultnya false
 hive.enforce.bucketing                                       true   , (tidak ada di ver 2 always true)
 hive.exec.dynamic.partition.mode                        nonstrict   , defaultnya strict
 hive.txn.manager   org.apache.hadoop.hive.ql.lockmgr.DbTxnManager   , defaultnya org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager
 hive.compactor.initiator.on                                  true   , defaultnya false
 hive.compactor.worker.threads                                   1   , defaultnya 0



------- untuk transaksi
https://community.cloudera.com/t5/Community-Articles/Hive-ACID-Merge-by-Example/ta-p/245402

CREATE TABLE IF NOT EXISTS employee ( eid int, name String,
salary String, destination String)
COMMENT 'Employee details'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

CREATE TABLE IF NOT EXISTS employee_orc ( eid int, name String,
salary decimal, destination String)
STORED AS ORC ;

CREATE TABLE IF NOT EXISTS employee_orc_trx ( eid int, name String,
salary decimal, destination String)
PARTITIONED BY (trx_date string)
CLUSTERED BY (eid) into 5 buckets 
STORED AS ORC TBLPROPERTIES ('transactional'='true') ;

drop table employee_orc_trx;
drop table employee_orc;

CREATE TABLE IF NOT EXISTS employee_orc ( eid int, name String,
salary decimal, destination String)
STORED AS ORC ;

CREATE TABLE IF NOT EXISTS employee_orc_trx ( eid int, name String,
salary decimal, destination String)
CLUSTERED BY (destination) into 15 buckets 
STORED AS ORC TBLPROPERTIES ('transactional'='true') ;

---------------------v
CREATE TABLE IF NOT EXISTS employee_orc_trx ( eid int, name String,
salary decimal, destination String)
PARTITIONED BY (datestamp STRING) 
CLUSTERED BY (destination) into 256 buckets 
STORED AS ORC TBLPROPERTIES ('transactional'='true') ;
----------------------v

In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties


set hive.exec.reducers.bytes.per.reducer=1000000000 ;
set hive.exec.reducers.max=999;
set mapreduce.job.reduces=-1; <<<< jangan -1  minimal 1


----------------v
https://stackoverflow.com/questions/42969589/execution-error-return-code-2-from-org-apache-hadoop-hive-ql-exec-mr-mapredtask

SET hive.exec.dynamic.partition = true;
SET hive.exec.dynamic.partition.mode = nonstrict;
SET hive.exec.max.dynamic.partitions = 10000;   defaultnya 1000;
SET hive.exec.max.dynamic.partitions.pernode = 1000; defaultnya 100;
----------------v

------- untuk transaksi
INSERT INTO TABLE employee VALUES (1, 'eki', '25200','asdasdsa'); 
INSERT INTO TABLE employee_orc (eid,name,salary, destination) VALUES (1, 'eki', 25200,'asdasdsa'); 
INSERT INTO TABLE employee_orc_trx (eid,name,salary,destination) VALUES (1, 'eki', 25200,'asdasdsa'); 






INSERT INTO TABLE employee_orc_trx (eid,name,salary,destination) VALUES (1, 'eki', 25200,'asdasdsa'); 
-------------------v
INSERT INTO TABLE employee_orc_trx PARTITION (datestamp = '2014-09-23') (eid,name,salary,destination) VALUES (1, 'eki', 25200,'asdasdsa'); 
INSERT INTO TABLE employee_orc_trx PARTITION (datestamp) (eid,name,salary,destination) VALUES (2, 'eki', 25200,'asdasdsa'); 
-------------------v
from employee_orc
INSERT INTO TABLE employee_orc_trx
select eid,name,salary,destination

UPDATE employee_orc SET name = 'Eki Indradi', salary = 20000000 where eid= 1


https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.5/using-hiveql/content/hive_insert_data_in_a_hive_table.html


--------------------------------------------------------------



UPDATE employee SET name = 'Eki Indradi', salary = '20000000' where eid= 1


sudo rm -r /home/<USER_HIDDEN>/tmpdata
sudo mkdir -p /home/<USER_HIDDEN>/tmpdata
--- not use   sudo chown <USER_HIDDEN>:hadoop /home/<USER_HIDDEN>/tmpdata
sudo chmod 777 /home/<USER_HIDDEN>/tmpdata


hdfs namenode -format
hdfs dfsadmin -safemode leave
hadoop namenode -format  << 2.7.7 pake ini


SET mapred.reduce.tasks=1 / SET mapreduce.job.reduces=1;
set hive.auto.convert.join.noconditionaltask=false;
----------------------
